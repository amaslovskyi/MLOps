{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a {subject} teacher\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{concept}\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model = \"llama3.1:latest\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"subject\": \"Chemistry\",\n",
    "        \"concept\": \"periodic table\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The periodic table! It's the foundation of chemistry, and it's so fascinating to explore.\n",
      "\n",
      "So, you want to know about the periodic table? Well, let me tell you that it's an organized way of displaying all the known elements. The elements are listed in order of their atomic number (number of protons in the nucleus), which increases from left to right and top to bottom.\n",
      "\n",
      "Here are some key features of the periodic table:\n",
      "\n",
      "1. **Metals**: On the left side and in the middle, you'll find the metals. These are elements that are typically shiny, malleable, and good conductors of electricity.\n",
      "2. **Nonmetals**: On the right side, you'll see the nonmetals. These elements are usually dull, brittle, and poor conductors of electricity.\n",
      "3. **Noble Gases**: At the far right, you'll find the noble gases (Group 18). These elements are unreactive and stable.\n",
      "4. **Transition Metals**: In the middle section, between metals and nonmetals, you'll find the transition metals. These elements have properties that lie between those of metals and nonmetals.\n",
      "\n",
      "Now, let's talk about some important groups:\n",
      "\n",
      "1. **Alkali Metals** (\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shot chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Ukraine\",\n",
    "        \"output\": \"eniarkU\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Canada\",\n",
    "        \"output\": \"adanaC\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Australia\",\n",
    "        \"output\": \"ailartsuA\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = examples_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Ukraine\n",
      "AI: eniarkU\n",
      "Human: Canada\n",
      "AI: adanaC\n",
      "Human: Australia\n",
      "AI: ailartsuA\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a linguistic scpecialist\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format_messages(input=\"Zurich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model = \"llama3.1:latest\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hciruz'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
