{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This pipeline processes both the text content of the PDF and any images it contains, allowing for a more comprehensive understanding of the document. The OCR functionality enables the system to extract text from images, which is then included in the knowledge base used for answering queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq langchain-community\n",
    "%pip install -Uq pytesseract frontend\n",
    "%pip install -Uq fitz PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import io\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# LLM and embeddings setup\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0,\n",
    "    # num_predict = 256,\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract text and images from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/Shatten/Edu/Python/MLOps/eBook.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_content = \"\"\n",
    "    images = []\n",
    "\n",
    "    for page in doc:\n",
    "        text_content += page.get_text()\n",
    "        \n",
    "        for img in page.get_images(full=True):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            images.append(image)\n",
    "\n",
    "    return text_content, images\n",
    "\n",
    "# Example usage:\n",
    "# pdf_path = \"path/to/your/pdf/file.pdf\"\n",
    "# text, images = process_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, images = process_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a function to extract text from images using optical character recognition (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_images(images):\n",
    "    image_texts = []\n",
    "    for img in images:\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        image_texts.append(text)\n",
    "    return \"\\n\".join(image_texts)\n",
    "\n",
    "# Example usage:\n",
    "# image_text = extract_text_from_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_text = extract_text_from_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_pipeline(pdf_path):\n",
    "    # Process PDF\n",
    "    text_content, images = process_pdf(pdf_path)\n",
    "    \n",
    "    # Extract text from images\n",
    "    image_text = extract_text_from_images(images)\n",
    "    \n",
    "    # Combine all text\n",
    "    all_text = text_content + \"\\n\" + image_text\n",
    "    \n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = Chroma.from_texts(chunks, embeddings)\n",
    "    \n",
    "    # Create memory\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    \n",
    "    # Create retrieval chain\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "# Example usage:\n",
    "# pdf_path = \"path/to/your/pdf/file.pdf\"\n",
    "# qa_chain = create_rag_pipeline(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path = \"/Users/Shatten/Edu/Python/MLOps/eBook.pdf\"\n",
    "# qa_chain = create_rag_pipeline(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(qa_chain, query):\n",
    "    result = qa_chain({\"question\": query})\n",
    "    return result['answer'], qa_chain\n",
    "\n",
    "# Example usage:\n",
    "# pdf_path = \"path/to/your/pdf/file.pdf\"\n",
    "# qa_chain = create_rag_pipeline(pdf_path)\n",
    "# query = \"What is the main topic of this PDF?\"\n",
    "# answer = query_rag(qa_chain, query)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is the Foundational machine learning skills in this PDF?\n",
      "A1: The text doesn't explicitly mention \"Foundational machine learning skills\". However, it does mention that understanding the math behind algorithms can be helpful for debugging them, specifically mentioning linear algebra libraries for solving linear systems of equations (for linear regression) and gradient descent, momentum, and the Adam optimization algorithm in deep learning.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"/Users/Shatten/Edu/Python/MLOps/eBook.pdf\"\n",
    "qa_chain = create_rag_pipeline(pdf_path)\n",
    "query = \"What is the Foundational machine learning skills in this PDF?\"\n",
    "answer, qa_chain = query_rag(qa_chain, query)\n",
    "print(\"Q1:\", query)\n",
    "print(\"A1:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
