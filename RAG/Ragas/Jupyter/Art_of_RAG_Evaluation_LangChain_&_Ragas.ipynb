{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa8ykQk92aLX"
   },
   "source": [
    "# Evaluation with RAGAS and Advanced Retrieval Methods Using LangChain\n",
    "\n",
    "In the following notebook we'll discuss a major component of LLM Ops:\n",
    "\n",
    "- Evaluation\n",
    "\n",
    "We're going to be leveraging the [RAGAS]() framework for our evaluations today as it's becoming a standard method of evaluating (at least directionally) RAG systems.\n",
    "\n",
    "We're also going to discuss a few more powerful Retrieval Systems that can potentially improve the quality of our generations!\n",
    "\n",
    "Let's start as we always do: Grabbing our dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5BN13TZlSCv4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q langchain langchain-ollama ragas arxiv pymupdf chromadb wandb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Lhqp5rUThG-",
    "outputId": "c33f7eee-b819-40bd-dc75-ce90721a6a94"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from operator import itemgetter\n",
    "# import openai\n",
    "from getpass import getpass\n",
    "\n",
    "# openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV_BOewX8CW0"
   },
   "source": [
    "### Data Collection\n",
    "\n",
    "We're going to be using papers from Arxiv as our context today.\n",
    "\n",
    "We can collect these documents rather straightforwardly with the `ArxivLoader` document loader from LangChain.\n",
    "\n",
    "Let's grab and load 5 documents.\n",
    "\n",
    "- [`ArxivLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.arxiv.ArxivLoader.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTDNFXaBSO2j",
    "outputId": "3b24521d-5c6f-466b-d818-46ce68d359ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "base_docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n",
    "len(base_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNPAWPgNSyGP",
    "outputId": "b2f80fc8-792c-489a-b8d4-9f98678c679a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Published': '2024-10-30', 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation', 'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen', 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\"}\n",
      "{'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}\n",
      "{'Published': '2024-07-04', 'Title': 'Meta-prompting Optimized Retrieval-augmented Generation', 'Authors': 'João Rodrigues, António Branco', 'Summary': 'Retrieval-augmented generation resorts to content retrieved from external\\nsources in order to leverage the performance of large language models in\\ndownstream tasks. The excessive volume of retrieved content, the possible\\ndispersion of its parts, or their out of focus range may happen nevertheless to\\neventually have a detrimental rather than an incremental effect. To mitigate\\nthis issue and improve retrieval-augmented generation, we propose a method to\\nrefine the retrieved content before it is included in the prompt by resorting\\nto meta-prompting optimization. Put to empirical test with the demanding\\nmulti-hop question answering task from the StrategyQA dataset, the evaluation\\nresults indicate that this method outperforms a similar retrieval-augmented\\nsystem but without this method by over 30%.'}\n"
     ]
    }
   ],
   "source": [
    "for doc in base_docs:\n",
    "  print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7ht6bJX9PAY"
   },
   "source": [
    "### Creating an Index\n",
    "\n",
    "Let's use a naive index creation strategy of just using `RecursiveCharacterTextSplitter` on our documents and embedding each into our `VectorStore` using `OpenAIEmbeddings()`.\n",
    "\n",
    "- [`RecursiveCharacterTextSplitter()`](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html)\n",
    "- [`Chroma`](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html?highlight=chroma#langchain.vectorstores.chroma.Chroma)\n",
    "- [`OpenAIEmbeddings()`](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html?highlight=openaiembeddings#langchain-embeddings-openai-openaiembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xne8P5dQTUiR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shat_\\AppData\\Local\\Temp\\ipykernel_320\\2306905172.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  ollama_embeddings = OllamaEmbeddings(model=\"llama3.2-vision\", temperature=0.9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 500, {\"error\":{}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m ollama_embeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2-vision\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create the vector store using Ollama embeddings\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mollama_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize Ollama LLM\u001b[39;00m\n\u001b[0;32m     18\u001b[0m ollama_llm \u001b[38;5;241m=\u001b[39m OllamaLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2-vision\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    886\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m    888\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    889\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    890\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    891\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    892\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    893\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[0;32m    894\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[0;32m    895\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m    896\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    898\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[0;32m    838\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    839\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    840\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    841\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    842\u001b[0m     ):\n\u001b[1;32m--> 843\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    849\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\embeddings\\ollama.py:214\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m instruction_pairs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m--> 214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_emb_response(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\embeddings\\ollama.py:176\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;241m%\u001b[39m (res\u001b[38;5;241m.\u001b[39mstatus_code, res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    179\u001b[0m     )\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     t \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mValueError\u001b[0m: Error raised by inference API HTTP code: 500, {\"error\":{}}"
     ]
    }
   ],
   "source": [
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "\n",
    "docs = text_splitter.split_documents(base_docs)\n",
    "\n",
    "# vectorstore = Chroma.from_documents(docs, OllamaEmbeddings())\n",
    "\n",
    "# Initialize Ollama embeddings\n",
    "ollama_embeddings = OllamaEmbeddings(model=\"llama3.2-vision\", temperature=0.9)\n",
    "\n",
    "# Create the vector store using Ollama embeddings\n",
    "vectorstore = Chroma.from_documents(docs, ollama_embeddings)\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "ollama_llm = OllamaLLM(model=\"llama3.2-vision\", temperature=0.9)\n",
    "\n",
    "# Create the base retriever\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Update the retrieval_augmented_qa_chain to use Ollama LLM\n",
    "retrieval_augmented_qa_chain = (\n",
    "    RunnableParallel({\n",
    "        'context': itemgetter('question') | base_retriever,\n",
    "        'question': RunnablePassthrough()\n",
    "    }) | {\n",
    "        'response': prompt | ollama_llm | StrOutputParser(),\n",
    "        'context': itemgetter('context')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnRzYx4c_2mZ",
    "outputId": "59d9bdd8-0414-4e8b-c285-bf3a2760e26a"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyUh8EVI_6TZ",
    "outputId": "643fca9d-77c0-4296-d953-ec62d6de8954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "print(max([len(chunk.page_content) for chunk in docs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f9kNIUUTxdT"
   },
   "source": [
    "Let's convert our `Chroma` vectorstore into a retriever with the `.as_retriever()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bwbdftltT29h"
   },
   "outputs": [],
   "source": [
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBPZQUt4UBPl"
   },
   "source": [
    "Now to give it a test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r0Pie4xqUCkW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/5skqq8p925lbtt_m8d7msnkc0000gn/T/ipykernel_7381/3147441568.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = base_retriever.get_relevant_documents(\"What is Retrieval Augmented Generation?\")\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = base_retriever.get_relevant_documents(\"What is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_CiHTD0UKj7",
    "outputId": "fab040cf-971f-440a-a6aa-93873c8e152f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8MKsT6JTgCU"
   },
   "source": [
    "## Creating a Retrieval Augmented Generation Prompt\n",
    "\n",
    "Now we can set up a prompt template that will be used to provide the LLM with the necessary contexts, user query, and instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ijSNkTAjTsep"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYHnPaXl-cvJ"
   },
   "source": [
    "### Setting Up our Basic QA Chain\n",
    "\n",
    "Now we can instantiate our basic RAG chain!\n",
    "\n",
    "We'll follow *exactly* the chain we made on Tuesday to keep things simple for now - if you need a refresher on what it looked like - check out last week's notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-TsjUWjbUfbW"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "primary_qa_llm = ChatOllama(model=\"llama3.2-vision\", temperature=0)\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO69de-F-oMD"
   },
   "source": [
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FS5NxC6UyU2",
    "outputId": "2520bf73-9e62-435c-a213-c26d0655a913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': AIMessage(content='Retrieval Augmented Generation.', additional_kwargs={}, response_metadata={'model': 'llama3.2-vision', 'created_at': '2024-12-23T13:40:27.060249Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5993951750, 'load_duration': 32725334, 'prompt_eval_count': 858, 'prompt_eval_duration': 5601000000, 'eval_count': 8, 'eval_duration': 357000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-ed4cc78f-b3f8-4a1a-b811-cf1ef3bef1e6-0', usage_metadata={'input_tokens': 858, 'output_tokens': 8, 'total_tokens': 866}), 'context': [Document(metadata={'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen', 'Published': '2024-10-30', 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\", 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation'}, page_content='question using only the provided search\\nresults (some of which might be irrelevant).\\nOnly give me the answer and do not output\\nany other words.\\n{#Compressed documents}\\nOnly give me the answer and do not output\\nany other words.\\nQuestion: {#q}\\nAnswer:'), Document(metadata={'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen', 'Published': '2024-10-30', 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\", 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation'}, page_content='Comps\\nWrite a high-quality answer for the given\\nquestion using only the provided search\\nresults (some of which might be irrelevant).\\nOnly give me the answer and do not output\\nany other words.\\n{#Compressed documents}')]}\n"
     ]
    }
   ],
   "source": [
    "question = \"What is RAG?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyazkAIu85KL"
   },
   "source": [
    "### Ground Truth Dataset Creation Using GPT-3.5-turbo(mistral-nemo) and GPT-4 (llama3.2)\n",
    "\n",
    "The next section might take you a long time to run, so the evaluation dataset is provided.\n",
    "\n",
    "The basic idea is that we can use LangChain to create questions based on our contexts, and then answer those questions.\n",
    "\n",
    "Let's look at how that works in the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V24T_gpPatAO"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"question\",\n",
    "    description=\"a question about the context.\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ebbmazGrdPap"
   },
   "outputs": [],
   "source": [
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "format_instructions = question_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qorL4TPGXJQ7"
   },
   "outputs": [],
   "source": [
    "question_generation_llm = ChatOllama(model=\"mistral-nemo\")\n",
    "\n",
    "bare_prompt_template = \"{content}\"\n",
    "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oPqC1_MXdRuh"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\n",
    "\n",
    "question: a question about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "question\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=docs[0],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "question_generation_chain = bare_template | question_generation_llm\n",
    "\n",
    "response = question_generation_chain.invoke({\"content\" : messages})\n",
    "output_dict = question_output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKFw9kyZd7eB",
    "outputId": "bbcf9e15-58be-4899-f102-6cae59c45eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "How does the proposed 'R^2AG' framework address the semantic gap between Large Language Models (LLMs) and retrievers?\n",
      "context\n",
      "{'page_content': 'R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\\nFuda Ye1, Shuangyin Li1,*, Yongqi Zhang2, Lei Chen2,3\\n1School of Computer Science, South China Normal University', 'metadata': {'Published': '2024-10-30', 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation', 'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen', 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to augment large language models (LLMs) with external documents provided by retrievers. However, a semantic gap exists between LLMs and retrievers due to differences in their training objectives and architectures. This misalignment forces LLMs to passively accept the documents provided by the retrievers, leading to incomprehension in the generation process, where the LLMs are burdened with the task of distinguishing these documents using their inherent knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill this gap by incorporating Retrieval information into Retrieval Augmented Generation. Specifically, R$^2$AG utilizes the nuanced features from the retrievers and employs a R$^2$-Former to capture retrieval information. Then, a retrieval-aware prompting strategy is designed to integrate retrieval information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios where LLMs and retrievers are frozen. Extensive experiments across five datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our analysis reveals that retrieval information serves as an anchor to aid LLMs in the generation process, thereby filling the semantic gap.\"}}\n"
     ]
    }
   ],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dtASDdhLfd89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zolpr3CYeEYm",
    "outputId": "a7962cf2-4cdf-4c7a-b776-a0c2478042e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [05:12<00:00, 31.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "qac_triples = []\n",
    "\n",
    "for text in tqdm(docs[:10]):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=text,\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = question_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = question_output_parser.parse(response.content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  output_dict[\"context\"] = text\n",
    "  qac_triples.append(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKBdQHK7Y2Vw",
    "outputId": "73c7d139-be2d-483f-9f70-b6c4aae91506"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the primary goal of the proposed method R^2AG?',\n",
       " 'context': Document(metadata={'Published': '2024-10-30', 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation', 'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen', 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\"}, page_content='Abstract\\nRetrieval augmented generation (RAG) has\\nbeen applied in many scenarios to augment\\nlarge language models (LLMs) with external\\ndocuments provided by retrievers. However,\\na semantic gap exists between LLMs and')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qac_triples[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vNB9Z2DrX2TC"
   },
   "outputs": [],
   "source": [
    "answer_generation_llm = ChatOllama(model=\"llama3.2-vision\", temperature=0)\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "format_instructions = answer_output_parser.get_format_instructions()\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=qac_triples[0][\"context\"],\n",
    "    question=qac_triples[0][\"question\"],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "answer_generation_chain = bare_template | answer_generation_llm\n",
    "\n",
    "response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "output_dict = answer_output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rk-_lRR6fn5U",
    "outputId": "fb014a65-a56f-49be-8ecf-9ca5527aa803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\n",
      "The enhanced RAG framework proposed in the paper, R^2AG, addresses the 'semantic gap' between Large Language Models (LLMs) and retrievers by incorporating retrieval information into the generation process. Specifically, it utilizes nuanced features from the retrievers and employs a R^2-Former to capture retrieval information, which is then integrated into LLMs' generation using a retrieval-aware prompting strategy. This approach fills the semantic gap by providing an anchor for LLMs to aid in the generation process, making it more effective, robust, and efficient.\n"
     ]
    }
   ],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCdH0e9rrAKd",
    "outputId": "629d8791-dedb-47c7-b5a0-adaa26f142cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9/9 [02:11<00:00, 14.60s/it]\n"
     ]
    }
   ],
   "source": [
    "for triple in tqdm(qac_triples):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=triple[\"context\"],\n",
    "      question=triple[\"question\"],\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = answer_output_parser.parse(response.content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  triple[\"answer\"] = output_dict[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rrHXgH9Qs1ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uAvGGTyXsoHQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
    "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
    "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
    "\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_FHUnAPVseB",
    "outputId": "1a907389-e62b-4686-b3d7-e8707acfbd47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'ground_truth'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8CaCUeBVu4l",
    "outputId": "72cbf3c8-c698-4682-821a-8566f36f6adb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Compare and contrast Retrieval Augmented Generation (RAG) with the enhanced RAG framework proposed in the paper. How does the novel approach address the 'semantic gap' between Large Language Models (LLMs) and retrievers?\",\n",
       " 'context': 'R2AG: Incorporating Retrieval Information into Retrieval Augmented\\nGeneration\\nFuda Ye1, Shuangyin Li1,*, Yongqi Zhang2, Lei Chen2,3\\n1School of Computer Science, South China Normal University',\n",
       " 'ground_truth': \"The enhanced RAG framework proposed in the paper, R^2AG, addresses the 'semantic gap' between Large Language Models (LLMs) and retrievers by incorporating retrieval information into the generation process. Specifically, it utilizes nuanced features from the retrievers and employs a R^2-Former to capture retrieval information, which is then integrated into LLMs' generation using a retrieval-aware prompting strategy. This approach fills the semantic gap by providing an anchor for LLMs to aid in the generation process, making it more effective, robust, and efficient.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "c4bc64d47f2e4a239cf7156e7812887d",
      "b4a71cf676584200b46922fb976d1d50",
      "65f75f3d72cd415faffb32999893738d",
      "5367cbe7ca4d43b089ff0d5d7cd17d3c",
      "31390facfbeb455fa83067fc23d30718",
      "22fc25617c664e4aa02b7457832c1bef",
      "b556054df18d47aa8cb58ac482ca31bb",
      "0fff4a6672c848a38945e19101c46168",
      "de662cb67d054f08a15d191923d40369",
      "b53ebcf672a244978391cae559db5bf2",
      "f2de6a2c1f4b4a2fa5196028c0da0754"
     ]
    },
    "id": "Nhp5X4M8zqrm",
    "outputId": "7e3c36df-12a3-4ea7-a772-dc1e0bc61568"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e528086ba7d43078d45a669f0ac9e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6110"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.to_csv(\"groundtruth_eval_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Al5cagr-rvL"
   },
   "source": [
    "### Evaluating RAG Pipelines\n",
    "\n",
    "If you skipped ahead and need to load the `.csv` directly - uncomment the code below.\n",
    "\n",
    "If you're using Colab to do this notebook - please ensure you add it to your session files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJhes58R66-P"
   },
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# eval_dataset = Dataset.from_csv(\"groundtruth_eval_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fAD8c_kthWc",
    "outputId": "e722498d-3179-4cf1-e206-29a27163ace5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'ground_truth'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqFYbjLK-6X7"
   },
   "source": [
    "### Evaluation Using RAGAS\n",
    "\n",
    "Now we can evaluate using RAGAS!\n",
    "\n",
    "The set-up is fairly straightforward - we simply need to create a dataset with our generated answers and our contexts, and then evaluate using the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragas in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (0.2.8)\n",
      "Requirement already satisfied: numpy in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (1.26.4)\n",
      "Requirement already satisfied: datasets in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (3.2.0)\n",
      "Requirement already satisfied: tiktoken in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (0.8.0)\n",
      "Requirement already satisfied: langchain in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (0.3.13)\n",
      "Requirement already satisfied: langchain-core in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (0.3.28)\n",
      "Requirement already satisfied: langchain-community in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (0.3.12)\n",
      "Requirement already satisfied: langchain_openai in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (0.2.12)\n",
      "Requirement already satisfied: nest-asyncio in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (1.5.6)\n",
      "Requirement already satisfied: appdirs in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (2.10.3)\n",
      "Requirement already satisfied: openai>1 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (1.57.4)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from openai>1->ragas) (3.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from openai>1->ragas) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from openai>1->ragas) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from openai>1->ragas) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from openai>1->ragas) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from openai>1->ragas) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pydantic>=2->ragas) (2.27.1)\n",
      "Requirement already satisfied: filelock in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (0.24.2)\n",
      "Requirement already satisfied: packaging in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets->ragas) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain->ragas) (2.0.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain->ragas) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain->ragas) (0.2.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain->ragas) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain-community->ragas) (2.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from tiktoken->ragas) (2024.7.24)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain->ragas) (3.10.6)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain->ragas) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests>=2.32.2->datasets->ragas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pandas->datasets->ragas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pandas->datasets->ragas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pandas->datasets->ragas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "1eBoHaf5t4w8"
   },
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    # context_precision,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "# from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
    "  rag_dataset = []\n",
    "  for row in tqdm(eval_dataset):\n",
    "    answer = rag_pipeline.invoke({\"question\" : row[\"question\"]})\n",
    "    rag_dataset.append(\n",
    "        {\"question\" : row[\"question\"],\n",
    "         \"answer\" : answer[\"response\"].content,\n",
    "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
    "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
    "         }\n",
    "    )\n",
    "  rag_df = pd.DataFrame(rag_dataset)\n",
    "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "  return rag_eval_dataset\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        # context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4c4Jd8G_lXY"
   },
   "source": [
    "Lets create our dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7oXgcjkuopO",
    "outputId": "6db1a904-90a2-4e47-85da-a442ebdc56b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:17<00:00,  8.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "basic_qa_ragas_dataset = create_ragas_dataset(retrieval_augmented_qa_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfzaFWEMZ5l_",
    "outputId": "60ca3e05-209a-4375-f24c-a23ad06e525e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_ragas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Vv1NsRGZ6m5",
    "outputId": "2ef4fbd9-011e-42b7-88d9-11c79974d87e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Compare and contrast Retrieval Augmented Generation (RAG) with the enhanced RAG framework proposed in the paper. How does the novel approach address the 'semantic gap' between Large Language Models (LLMs) and retrievers?\",\n",
       " 'answer': \"I don't know\",\n",
       " 'contexts': ['This \"brute force\" optimization approach, scoring 25.92%, is outperformed\\nnot only by the proposed method of meta-prompting optimization, with 34.69%,\\nbut even also by the baseline, plain RAG, with 26.12%.',\n",
       "  'Put to empirical test with the demanding multi-hop question answer-\\ning task from the StrategyQA dataset, the evaluation results indicate\\nthat this method outperforms a similar retrieval-augmented system but\\nwithout this method by over 30 %.'],\n",
       " 'ground_truths': [\"The enhanced RAG framework proposed in the paper, R^2AG, addresses the 'semantic gap' between Large Language Models (LLMs) and retrievers by incorporating retrieval information into the generation process. Specifically, it utilizes nuanced features from the retrievers and employs a R^2-Former to capture retrieval information, which is then integrated into LLMs' generation using a retrieval-aware prompting strategy. This approach fills the semantic gap by providing an anchor for LLMs to aid in the generation process, making it more effective, robust, and efficient.\"]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_ragas_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Obbgw3im_n01"
   },
   "source": [
    "Save it for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "86761a36bdb04fed9f1dae6e74da54ce",
      "e079c0eb3d9c4ae3a40491f059a864f7",
      "d4bde97f3895450782030cad7808ea59",
      "1d17b8847ab34a1d8fc79430e8b64a12",
      "fb953d8d116e4cc389dbc23a0055873f",
      "bf27f78edb15477492fb2d5dd8dc5137",
      "ee96a569b8a54944bcac1b1bc164e53e",
      "e556e254882340168833ecf1a7153a90",
      "c077837b8b044c2fb14dcd8fbc44a37b",
      "e5c8b5933c754a1192456f43682276c5",
      "84abf77081a04bb686f794b49e04761d"
     ]
    },
    "id": "6FG8x4i3yZ2B",
    "outputId": "52eb909f-69be-4c80-d9bc-77c2842bf14d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d37e3156a24d7c8e94bc530549e80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11976"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_ragas_dataset.to_csv(\"basic_qa_ragas_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5I_d_RT_pFr"
   },
   "source": [
    "And finally - evaluate how it did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywp3Rwavy9pc",
    "outputId": "7a3948ce-b743-4d95-a581-17e035215f3f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The metric [context_recall] that is used requires the following additional columns ['reference'] to be present in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m basic_qa_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ragas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_qa_ragas_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 29\u001b[0m, in \u001b[0;36mevaluate_ragas_dataset\u001b[0;34m(ragas_dataset)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_ragas_dataset\u001b[39m(ragas_dataset):\n\u001b[0;32m---> 29\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mragas_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# context_precision,\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_recall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer_correctness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer_similarity\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ragas/_analytics.py:205\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    204\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 205\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ragas/evaluation.py:183\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[1;32m    180\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m EvaluationDataset\u001b[38;5;241m.\u001b[39mfrom_list(dataset\u001b[38;5;241m.\u001b[39mto_list())\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, EvaluationDataset):\n\u001b[0;32m--> 183\u001b[0m     \u001b[43mvalidate_required_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     validate_supported_metrics(dataset, metrics)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ragas/validation.py:63\u001b[0m, in \u001b[0;36mvalidate_required_columns\u001b[0;34m(ds, metrics)\u001b[0m\n\u001b[1;32m     61\u001b[0m available_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(ds\u001b[38;5;241m.\u001b[39mfeatures())\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_columns\u001b[38;5;241m.\u001b[39missubset(available_columns):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe metric [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] that is used requires the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(required_columns\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mavailable_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be present in the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The metric [context_recall] that is used requires the following additional columns ['reference'] to be present in the dataset."
     ]
    }
   ],
   "source": [
    "basic_qa_result = evaluate_ragas_dataset(basic_qa_ragas_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4oYnKTn15gY",
    "outputId": "ba48e4d2-5559-4748-8bc6-0101074a5c0e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basic_qa_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbasic_qa_result\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'basic_qa_result' is not defined"
     ]
    }
   ],
   "source": [
    "basic_qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwhBxlxYAdno"
   },
   "source": [
    "### Testing Other Retrievers\n",
    "\n",
    "Now we can test our how changing our Retriever impacts our RAGAS evaluation!\n",
    "\n",
    "We'll build this simple qa_chain factory to create standardized qa_chains where the only different component will be the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "qnfy4VNkzZi2"
   },
   "outputs": [],
   "source": [
    "def create_qa_chain(retriever):\n",
    "  primary_qa_llm = ChatOllama(model=\"mistral-nemo\", temperature=0)\n",
    "  created_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever,\n",
    "     \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | RunnablePassthrough.assign(\n",
    "        context=itemgetter(\"context\")\n",
    "      )\n",
    "    | {\n",
    "         \"response\": prompt | primary_qa_llm,\n",
    "         \"context\": itemgetter(\"context\"),\n",
    "      }\n",
    "  )\n",
    "\n",
    "  return created_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOPp4Xq7AvEx"
   },
   "source": [
    "#### Parent Document Retriever\n",
    "\n",
    "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that \"surrounds\" the found context.\n",
    "\n",
    "You can read more about this method [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)!\n",
    "\n",
    "The basic outline of this retrieval method is as follows:\n",
    "\n",
    "1. Obtain User Question\n",
    "2. Retrieve child documents using Dense Vector Retrieval\n",
    "3. Merge the child documents based on their parents. If they have the same parents - they become merged.\n",
    "4. Replace the child documents with their respective parent documents from an in-memory-store.\n",
    "5. Use the parent documents to augment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "67I6QJAJ0Un7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/5skqq8p925lbtt_m8d7msnkc0000gn/T/ipykernel_7381/3771999540.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=OllamaEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=OllamaEmbeddings())\n",
    "\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "zfk5RYUt00Pw"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "68c1t4o104AK"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 404, {\"error\":\"model \\\"llama2\\\" not found, try pulling it first\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparent_document_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain/retrievers/parent_document_retriever.py:133\u001b[0m, in \u001b[0;36mParentDocumentRetriever.add_documents\u001b[0;34m(self, documents, ids, add_to_docstore, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Adds documents to the docstore and vectorstores.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        and you don't want to re-add them.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m docs, full_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_docs_for_adding(documents, ids, add_to_docstore)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_to_docstore:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39mmset(full_docs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:287\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    286\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain_community/embeddings/ollama.py:214\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m instruction_pairs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m--> 214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain_community/embeddings/ollama.py:202\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain_community/embeddings/ollama.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain_community/embeddings/ollama.py:176\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;241m%\u001b[39m (res\u001b[38;5;241m.\u001b[39mstatus_code, res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    179\u001b[0m     )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     t \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference API HTTP code: 404, {\"error\":\"model \\\"llama2\\\" not found, try pulling it first\"}"
     ]
    }
   ],
   "source": [
    "parent_document_retriever.add_documents(base_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTH0MDolBndm"
   },
   "source": [
    "Let's create, test, and then evaluate our new chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMjLfqOC09Iw"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Rv8bAHPN1H4P",
    "outputId": "faa6bf43-1604-4468-9faf-bbefd8e48281"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'RAG stands for Retrieval-Augmented Generation.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQJRIQmU1WTw",
    "outputId": "295a9011-684d-4c38-e409-867022603608"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "pdr_qa_ragas_dataset = create_ragas_dataset(parent_document_retriever_qa_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "bf7f045bdbe24360ad2aa2f4c8f02e79",
      "761e3c6035bf49429b3035145451d2df",
      "ffb7c97e7af648aaa13a43427154140e",
      "95d92c5c74e845779337eb727c2bbfc0",
      "6e7fb9a1d1454fcd9bcf5b5f748fb975",
      "207785da1f404d6ea0e8c63655a7aa51",
      "71237d176b2c4138a5e0346d10482257",
      "ed457547dc154f6bbce4ec970bb09c76",
      "c9f479f81119450bb451d5830361467c",
      "56098a347ea94ea3b10bd8d2ec0d4288",
      "7940d9e3f5fa4592b58dec3fdb55595a"
     ]
    },
    "id": "d9vfKnCL1jtB",
    "outputId": "1d7421a8-b564-4da3-cd74-d1eb3f8311f7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7f045bdbe24360ad2aa2f4c8f02e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "55620"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdr_qa_ragas_dataset.to_csv(\"pdr_qa_ragas_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfB1H9S_1mW3",
    "outputId": "426d7b1b-2b0c-40d3-e7d7-39da43363f06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:01<00:00, 61.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:09<00:00, 69.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:04<00:00, 64.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_correctness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "pdr_qa_result = evaluate_ragas_dataset(pdr_qa_ragas_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nFyYCdL2Nco",
    "outputId": "bdde7173-c649-40bc-cbc3-e38a70c9f50a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.6972, 'faithfulness': 0.3500, 'answer_relevancy': 0.9439, 'context_recall': 1.0000, 'context_relevancy': 0.0134, 'answer_correctness': 0.6000, 'answer_similarity': 1.0000}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdr_qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaNk6o7_BqX8"
   },
   "source": [
    "#### Ensemble Retrieval\n",
    "\n",
    "Next let's look at ensemble retrieval!\n",
    "\n",
    "You can read more about this [here](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)!\n",
    "\n",
    "The basic idea is as follows:\n",
    "\n",
    "1. Obtain User Question\n",
    "2. Hit the Retriever Pair\n",
    "    - Retrieve Documents with BM25 Sparse Vector Retrieval\n",
    "    - Retrieve Documents with Dense Vector Retrieval Method\n",
    "3. Collect and \"fuse\" the retrieved docs based on their weighting using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm into a single ranked list.\n",
    "4. Use those documents to augment our generation.\n",
    "\n",
    "Ensure your `weights` list - the relative weighting of each retriever - sums to 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz7dl1GD5-L-"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vs8wxT9b5pRA"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=75)\n",
    "docs = text_splitter.split_documents(base_docs)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embedding)\n",
    "chroma_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, chroma_retriever], weights=[0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv69YDpF6PrJ"
   },
   "outputs": [],
   "source": [
    "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6lSszzrf6UmP",
    "outputId": "8a5893d5-4095-42e5-aecf-66d849512321"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'RAG stands for Retrieval-Augmented Generation.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abUgTGDT6UrV",
    "outputId": "749ae6db-75b9-48a7-e743-a8aecdcbd802"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "ensemble_qa_ragas_dataset = create_ragas_dataset(ensemble_retriever_qa_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "632135599e39470aac1a3bb3d3de0ca4",
      "567176b50d074a1c9d384a8df8c3ff4c",
      "6bfa39f2c84e4f08b5ffb9a416398824",
      "911b40169865413b98643789150e5495",
      "2add9d30edd84152bb6d7bfa4ed2d910",
      "f34ffd11b98045b9bae326dd48a54896",
      "e3e19fc9963c4bf39293bda7c2030d5a",
      "c15c27b0523a429e9d77f439d47ada90",
      "256a0dbb2f104d928e181d2a882a9867",
      "68790cfe4ea14fc4a63275f3e99c468f",
      "e7df092ac205443e8baa3646ff1eae5b"
     ]
    },
    "id": "bGHipYsf7phk",
    "outputId": "a70b0d3e-870f-49b8-a16a-5b7d4623c33f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632135599e39470aac1a3bb3d3de0ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "22820"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_qa_ragas_dataset.to_csv(\"ensemble_qa_ragas_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ozo0jkvx7r1d",
    "outputId": "f5770c52-4614-4172-8834-d48bd4005218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:01<00:00, 61.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:08<00:00, 68.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:02<00:00, 62.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_correctness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  9.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "ensemble_qa_result = evaluate_ragas_dataset(ensemble_qa_ragas_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvabdcbh793a",
    "outputId": "56daa44b-841b-4924-9242-77c2bc93f86e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.8858, 'faithfulness': 0.7000, 'answer_relevancy': 0.8918, 'context_recall': 0.9800, 'context_relevancy': 0.0192, 'answer_correctness': 0.7750, 'answer_similarity': 1.0000}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4vXVWqiCcSI"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Observe your results in a table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmBoVQ5hV3Kc",
    "outputId": "12196187-5cbf-40b2-f35f-ab45616e71a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.5000, 'faithfulness': 0.4000, 'answer_relevancy': 0.9535, 'context_recall': 1.0000, 'context_relevancy': 0.0559, 'answer_correctness': 0.6167, 'answer_similarity': 1.0000}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ax1JLXKxUsXF",
    "outputId": "b83ba792-7e66-44ff-b219-0f3890a5fe8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.6972, 'faithfulness': 0.3500, 'answer_relevancy': 0.9439, 'context_recall': 1.0000, 'context_relevancy': 0.0134, 'answer_correctness': 0.6000, 'answer_similarity': 1.0000}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdr_qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drxLlO3zUpyQ",
    "outputId": "3b595607-2fa5-4590-d2e6-9707aa7bb283"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.8858, 'faithfulness': 0.7000, 'answer_relevancy': 0.8918, 'context_recall': 0.9800, 'context_relevancy': 0.0192, 'answer_correctness': 0.7750, 'answer_similarity': 1.0000}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6YPGf-2l0Kx"
   },
   "source": [
    "We can also zoom in on each result and find specific information about each of the questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkxLtk43ikka"
   },
   "outputs": [],
   "source": [
    "ensemble_qa_result_df = ensemble_qa_result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3JZMhUg3jSvE",
    "outputId": "95d1cf44-88ee-4056-b698-64237b119fe0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a1e0be28-2fa7-4b13-8d13-5b1c1554cdd9\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_relevancy</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the focus of this paper?</td>\n",
       "      <td>[has to make an important career decision.\\nNe...</td>\n",
       "      <td>The focus of this paper is on a framework call...</td>\n",
       "      <td>[The focus of this paper is on retrieval-augme...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.784617</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the title of the paper?</td>\n",
       "      <td>[of War. The game was released worldwide in\\nG...</td>\n",
       "      <td>Title: Self-RAG: Learning to Retrieve, Generat...</td>\n",
       "      <td>[The title of the paper is 'A Survey on Retrie...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the aim of this paper?</td>\n",
       "      <td>[has to make an important career decision.\\nNe...</td>\n",
       "      <td>The aim of this paper is to introduce a new fr...</td>\n",
       "      <td>[The aim of this paper is to conduct a compreh...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.800732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.75</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the main focus of the paper 'A Survey ...</td>\n",
       "      <td>[A Survey on Retrieval-Augmented Text Generati...</td>\n",
       "      <td>The main focus of the paper 'A Survey on Retri...</td>\n",
       "      <td>[The main focus of the paper 'A Survey on Retr...</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982435</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the main focus of this paper?</td>\n",
       "      <td>[example of completions of the prompt by diffe...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>[The main focus of this paper is to conduct a ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.742601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the main focus of this paper?</td>\n",
       "      <td>[example of completions of the prompt by diffe...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>[The main focus of this paper is to conduct a ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.742601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the advantages of retrieval-augmented...</td>\n",
       "      <td>[attracted increasing attention of the compu-\\...</td>\n",
       "      <td>The advantages of retrieval-augmented text gen...</td>\n",
       "      <td>[The advantages of retrieval-augmented text ge...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968712</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the main focus of the paper 'A Survey ...</td>\n",
       "      <td>[A Survey on Retrieval-Augmented Text Generati...</td>\n",
       "      <td>The main focus of the paper 'A Survey on Retri...</td>\n",
       "      <td>[The main focus of the paper 'A Survey on Retr...</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the advantages of retrieval-augmented...</td>\n",
       "      <td>[attracted increasing attention of the compu-\\...</td>\n",
       "      <td>The advantages of retrieval-augmented text gen...</td>\n",
       "      <td>[The advantages of retrieval-augmented text ge...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the advantages of retrieval-augmented...</td>\n",
       "      <td>[attracted increasing attention of the compu-\\...</td>\n",
       "      <td>The advantages of retrieval-augmented text gen...</td>\n",
       "      <td>[The advantages of retrieval-augmented text ge...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1e0be28-2fa7-4b13-8d13-5b1c1554cdd9')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a1e0be28-2fa7-4b13-8d13-5b1c1554cdd9 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a1e0be28-2fa7-4b13-8d13-5b1c1554cdd9');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-9d17e429-a85f-4eed-ab52-253849a35ce1\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d17e429-a85f-4eed-ab52-253849a35ce1')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-9d17e429-a85f-4eed-ab52-253849a35ce1 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                   What is the focus of this paper?   \n",
       "1                    What is the title of the paper?   \n",
       "2                     What is the aim of this paper?   \n",
       "3  What is the main focus of the paper 'A Survey ...   \n",
       "4              What is the main focus of this paper?   \n",
       "5              What is the main focus of this paper?   \n",
       "6  What are the advantages of retrieval-augmented...   \n",
       "7  What is the main focus of the paper 'A Survey ...   \n",
       "8  What are the advantages of retrieval-augmented...   \n",
       "9  What are the advantages of retrieval-augmented...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [has to make an important career decision.\\nNe...   \n",
       "1  [of War. The game was released worldwide in\\nG...   \n",
       "2  [has to make an important career decision.\\nNe...   \n",
       "3  [A Survey on Retrieval-Augmented Text Generati...   \n",
       "4  [example of completions of the prompt by diffe...   \n",
       "5  [example of completions of the prompt by diffe...   \n",
       "6  [attracted increasing attention of the compu-\\...   \n",
       "7  [A Survey on Retrieval-Augmented Text Generati...   \n",
       "8  [attracted increasing attention of the compu-\\...   \n",
       "9  [attracted increasing attention of the compu-\\...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The focus of this paper is on a framework call...   \n",
       "1  Title: Self-RAG: Learning to Retrieve, Generat...   \n",
       "2  The aim of this paper is to introduce a new fr...   \n",
       "3  The main focus of the paper 'A Survey on Retri...   \n",
       "4                                      I don't know.   \n",
       "5                                      I don't know.   \n",
       "6  The advantages of retrieval-augmented text gen...   \n",
       "7  The main focus of the paper 'A Survey on Retri...   \n",
       "8  The advantages of retrieval-augmented text gen...   \n",
       "9  The advantages of retrieval-augmented text gen...   \n",
       "\n",
       "                                       ground_truths  context_precision  \\\n",
       "0  [The focus of this paper is on retrieval-augme...           1.000000   \n",
       "1  [The title of the paper is 'A Survey on Retrie...           0.500000   \n",
       "2  [The aim of this paper is to conduct a compreh...           1.000000   \n",
       "3  [The main focus of the paper 'A Survey on Retr...           0.679167   \n",
       "4  [The main focus of this paper is to conduct a ...           1.000000   \n",
       "5  [The main focus of this paper is to conduct a ...           1.000000   \n",
       "6  [The advantages of retrieval-augmented text ge...           1.000000   \n",
       "7  [The main focus of the paper 'A Survey on Retr...           0.679167   \n",
       "8  [The advantages of retrieval-augmented text ge...           1.000000   \n",
       "9  [The advantages of retrieval-augmented text ge...           1.000000   \n",
       "\n",
       "   faithfulness  answer_relevancy  context_recall  context_relevancy  \\\n",
       "0      0.666667          0.784617             1.0           0.000000   \n",
       "1      1.000000          0.976911             1.0           0.000000   \n",
       "2      0.333333          0.800732             1.0           0.078947   \n",
       "3      1.000000          0.982435             0.8           0.017857   \n",
       "4      0.000000          0.742601             1.0           0.000000   \n",
       "5      0.000000          0.742601             1.0           0.000000   \n",
       "6      1.000000          0.968712             1.0           0.025641   \n",
       "7      1.000000          0.982422             1.0           0.017857   \n",
       "8      1.000000          0.968731             1.0           0.025641   \n",
       "9      1.000000          0.968692             1.0           0.025641   \n",
       "\n",
       "   answer_correctness  answer_similarity  \n",
       "0                0.50               True  \n",
       "1                0.50               True  \n",
       "2                0.75               True  \n",
       "3                1.00               True  \n",
       "4                0.50               True  \n",
       "5                0.50               True  \n",
       "6                1.00               True  \n",
       "7                1.00               True  \n",
       "8                1.00               True  \n",
       "9                1.00               True  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_qa_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jXR7ckel-v8"
   },
   "source": [
    "We'll also look at combining the results and looking at them in a single table so we can make inferences about them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5KKE_JkcD3"
   },
   "outputs": [],
   "source": [
    "def create_df_dict(pipeline_name, pipeline_items):\n",
    "  df_dict = {\"name\" : pipeline_name}\n",
    "  for name, score in pipeline_items:\n",
    "    df_dict[name] = score\n",
    "  return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1mPqYdqk4iQ"
   },
   "outputs": [],
   "source": [
    "basic_rag_df_dict = create_df_dict(\"basic_rag\", basic_qa_result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntJPzwy9lI46"
   },
   "outputs": [],
   "source": [
    "pdr_rag_df_dict = create_df_dict(\"pdr_rag\", pdr_qa_result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0fkbIQElPza"
   },
   "outputs": [],
   "source": [
    "ensemble_rag_df_dict = create_df_dict(\"ensemble_rag\", ensemble_qa_result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc4T1E83lVbE"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([basic_rag_df_dict, pdr_rag_df_dict, ensemble_rag_df_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "cv_9wNYGlibg",
    "outputId": "6580c17c-543a-4d54-8577-104c0173f368"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b21809b4-1506-4413-b95a-a7bdc7cf6816\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_relevancy</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ensemble_rag</td>\n",
       "      <td>0.885833</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.891845</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>basic_rag</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.953475</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.055904</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pdr_rag</td>\n",
       "      <td>0.697222</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.943909</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b21809b4-1506-4413-b95a-a7bdc7cf6816')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b21809b4-1506-4413-b95a-a7bdc7cf6816 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b21809b4-1506-4413-b95a-a7bdc7cf6816');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-43d2deee-1b4d-4900-9ce8-743ac419b9d1\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-43d2deee-1b4d-4900-9ce8-743ac419b9d1')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-43d2deee-1b4d-4900-9ce8-743ac419b9d1 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           name  context_precision  faithfulness  answer_relevancy  \\\n",
       "2  ensemble_rag           0.885833          0.70          0.891845   \n",
       "0     basic_rag           0.500000          0.40          0.953475   \n",
       "1       pdr_rag           0.697222          0.35          0.943909   \n",
       "\n",
       "   context_recall  context_relevancy  answer_correctness  answer_similarity  \n",
       "2            0.98           0.019158            0.775000                1.0  \n",
       "0            1.00           0.055904            0.616667                1.0  \n",
       "1            1.00           0.013386            0.600000                1.0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(\"answer_correctness\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPocfrNFiYWi"
   },
   "source": [
    "### ❓QUESTION❓\n",
    "\n",
    "What conclusions can you draw about the above results?\n",
    "\n",
    "Describe in your own words what the metrics are expressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbhz-vD4JPtN"
   },
   "outputs": [],
   "source": [
    "retrieval_augmented_qa_chain = (\n",
    "    RunnableParallel({\n",
    "        'context': itemgetter('question') | base_retriever,\n",
    "        'question': RunnablePassthrough()\n",
    "    }) | {\n",
    "        'response': prompt | primary_qa_llm | parser,\n",
    "        'context': itemgetter('context')\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fff4a6672c848a38945e19101c46168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d17b8847ab34a1d8fc79430e8b64a12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5c8b5933c754a1192456f43682276c5",
      "placeholder": "​",
      "style": "IPY_MODEL_84abf77081a04bb686f794b49e04761d",
      "value": " 1/1 [00:00&lt;00:00, 35.79ba/s]"
     }
    },
    "207785da1f404d6ea0e8c63655a7aa51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22fc25617c664e4aa02b7457832c1bef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "256a0dbb2f104d928e181d2a882a9867": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2add9d30edd84152bb6d7bfa4ed2d910": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31390facfbeb455fa83067fc23d30718": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5367cbe7ca4d43b089ff0d5d7cd17d3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b53ebcf672a244978391cae559db5bf2",
      "placeholder": "​",
      "style": "IPY_MODEL_f2de6a2c1f4b4a2fa5196028c0da0754",
      "value": " 1/1 [00:00&lt;00:00, 18.79ba/s]"
     }
    },
    "56098a347ea94ea3b10bd8d2ec0d4288": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "567176b50d074a1c9d384a8df8c3ff4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f34ffd11b98045b9bae326dd48a54896",
      "placeholder": "​",
      "style": "IPY_MODEL_e3e19fc9963c4bf39293bda7c2030d5a",
      "value": "Creating CSV from Arrow format: 100%"
     }
    },
    "632135599e39470aac1a3bb3d3de0ca4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_567176b50d074a1c9d384a8df8c3ff4c",
       "IPY_MODEL_6bfa39f2c84e4f08b5ffb9a416398824",
       "IPY_MODEL_911b40169865413b98643789150e5495"
      ],
      "layout": "IPY_MODEL_2add9d30edd84152bb6d7bfa4ed2d910"
     }
    },
    "65f75f3d72cd415faffb32999893738d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fff4a6672c848a38945e19101c46168",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de662cb67d054f08a15d191923d40369",
      "value": 1
     }
    },
    "68790cfe4ea14fc4a63275f3e99c468f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bfa39f2c84e4f08b5ffb9a416398824": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c15c27b0523a429e9d77f439d47ada90",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_256a0dbb2f104d928e181d2a882a9867",
      "value": 1
     }
    },
    "6e7fb9a1d1454fcd9bcf5b5f748fb975": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71237d176b2c4138a5e0346d10482257": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "761e3c6035bf49429b3035145451d2df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_207785da1f404d6ea0e8c63655a7aa51",
      "placeholder": "​",
      "style": "IPY_MODEL_71237d176b2c4138a5e0346d10482257",
      "value": "Creating CSV from Arrow format: 100%"
     }
    },
    "7940d9e3f5fa4592b58dec3fdb55595a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84abf77081a04bb686f794b49e04761d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86761a36bdb04fed9f1dae6e74da54ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e079c0eb3d9c4ae3a40491f059a864f7",
       "IPY_MODEL_d4bde97f3895450782030cad7808ea59",
       "IPY_MODEL_1d17b8847ab34a1d8fc79430e8b64a12"
      ],
      "layout": "IPY_MODEL_fb953d8d116e4cc389dbc23a0055873f"
     }
    },
    "911b40169865413b98643789150e5495": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68790cfe4ea14fc4a63275f3e99c468f",
      "placeholder": "​",
      "style": "IPY_MODEL_e7df092ac205443e8baa3646ff1eae5b",
      "value": " 1/1 [00:00&lt;00:00, 33.41ba/s]"
     }
    },
    "95d92c5c74e845779337eb727c2bbfc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56098a347ea94ea3b10bd8d2ec0d4288",
      "placeholder": "​",
      "style": "IPY_MODEL_7940d9e3f5fa4592b58dec3fdb55595a",
      "value": " 1/1 [00:00&lt;00:00, 33.86ba/s]"
     }
    },
    "b4a71cf676584200b46922fb976d1d50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22fc25617c664e4aa02b7457832c1bef",
      "placeholder": "​",
      "style": "IPY_MODEL_b556054df18d47aa8cb58ac482ca31bb",
      "value": "Creating CSV from Arrow format: 100%"
     }
    },
    "b53ebcf672a244978391cae559db5bf2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b556054df18d47aa8cb58ac482ca31bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf27f78edb15477492fb2d5dd8dc5137": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf7f045bdbe24360ad2aa2f4c8f02e79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_761e3c6035bf49429b3035145451d2df",
       "IPY_MODEL_ffb7c97e7af648aaa13a43427154140e",
       "IPY_MODEL_95d92c5c74e845779337eb727c2bbfc0"
      ],
      "layout": "IPY_MODEL_6e7fb9a1d1454fcd9bcf5b5f748fb975"
     }
    },
    "c077837b8b044c2fb14dcd8fbc44a37b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c15c27b0523a429e9d77f439d47ada90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4bc64d47f2e4a239cf7156e7812887d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4a71cf676584200b46922fb976d1d50",
       "IPY_MODEL_65f75f3d72cd415faffb32999893738d",
       "IPY_MODEL_5367cbe7ca4d43b089ff0d5d7cd17d3c"
      ],
      "layout": "IPY_MODEL_31390facfbeb455fa83067fc23d30718"
     }
    },
    "c9f479f81119450bb451d5830361467c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d4bde97f3895450782030cad7808ea59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e556e254882340168833ecf1a7153a90",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c077837b8b044c2fb14dcd8fbc44a37b",
      "value": 1
     }
    },
    "de662cb67d054f08a15d191923d40369": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e079c0eb3d9c4ae3a40491f059a864f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf27f78edb15477492fb2d5dd8dc5137",
      "placeholder": "​",
      "style": "IPY_MODEL_ee96a569b8a54944bcac1b1bc164e53e",
      "value": "Creating CSV from Arrow format: 100%"
     }
    },
    "e3e19fc9963c4bf39293bda7c2030d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e556e254882340168833ecf1a7153a90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5c8b5933c754a1192456f43682276c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7df092ac205443e8baa3646ff1eae5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed457547dc154f6bbce4ec970bb09c76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee96a569b8a54944bcac1b1bc164e53e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2de6a2c1f4b4a2fa5196028c0da0754": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f34ffd11b98045b9bae326dd48a54896": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb953d8d116e4cc389dbc23a0055873f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffb7c97e7af648aaa13a43427154140e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed457547dc154f6bbce4ec970bb09c76",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9f479f81119450bb451d5830361467c",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
